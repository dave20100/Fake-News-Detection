\chapter{Uczenie maszyn}
Uczenie maszynowe jest dziedziną algorytmów komputerowych, które automatycznie poprawiają swoją poprawność poprzez doświadczenie. 
Określa się je jako poddziedzinę sztucznej inteligencji. Algorytmy te pozwalają na zbudowanie matematycznego modelu na podstawie 
przykładowych danych nazywanych danymi treningowymi, co pozwala im na wykonywanie predykcji lub decyzji bez potrzeby ich dokładnego 
zaimplementowania przez programistę. 


Uczenie maszynowe stosuje się do wielu zadań, między innymi do filtrowania poczty mailowej ze spamu, reklam internetowe, 
wykrywania twarzy na zdjęciach oraz nagraniach, a w przyszłości może pomóc w stworzeniu technologii takich jak 
autonomiczne samochody. Sam termin został spopularyzowany przez informatyka Arhura Samuela w roku 1959, był on autorem pierwszego działającego 
systemu tego typu, jego program automatycznie grał w warcaby i uczył się na podstawie poprzednich potyczek ~\cite{MLBasics}. 


W dzisiejszych czasach implementacja takich systemów opiera się na wykonaniu następujących czynności:
\begin{itemize}
    \item preprocessing danych:
    \begin{itemize}
        \item czyszczenie,
        \item uzupełnianie,
        \item redukcja wymiarowości.
    \end{itemize}
    \item ekstrakcja cech,
    \item uczenie modelu,
    \item sprawdzenie poprawności modelu.
\end{itemize}
Odpowiednie wykonanie etapu ekstrakcji cech jest kluczowym elementem do stworzenia poprawnego modelu, jest
on opisany w kolejnym rozdziale pracy. 
\section{Rodzaje uczenia na podstawie przykładów}

Algorytmy uczenia maszynowego uczące się na podstawie przykładów można podzielić na dwa główne rodzaje w zależności od problemów, które mają one rozwiązywać:
\begin{itemize}
    \item uczenie nadzorowane -
    jest to najczęściej wykorzystywany rodzaj uczenia maszynowego, polega on na tym,
    że maszyna uczy się na podstawie przykładów zawartych w danych treningowych.
    Uczenie nadzorowane można porównać do nauczyciela i ucznia gdzie dane pełnią rolę nauczyciela, 
    a program ucznia. Algorytmy tego typu potrafią znaleźć odpowiednie zależności na podstawie
    etykiet przypisanym danym,
    które następnie wykorzystują w celu predykcji wcześniej nie analizowanych danych.
    Ważnym zagadnieniem w przypadku uczenia nadzorowanego jest tak zwany overfitting polegający 
    na przeuczeniu programu jednym zestawem treningowym, przez co traci on umiejętność generalizacji problemu
    i nie jest w stanie poprawnie podejmować predykcji danych niewystarczająco podobnych
    do treningowych.

    Przykładowe zastosowania:
    \begin{itemize}
        \item klasyfikacja - przewidywanie kategorii:
        \begin{itemize}
            \item rozpoznawanie elementów na zdjęciu,
            \item filtrowanie spamu w skrzynce mailowej.
        \end{itemize}
        \item regresja - przewidywanie liczb:
        \begin{itemize}
            \item przewidywanie trendów finansowych lub ekonomicznych,
            \item prognozowanie pogody.
        \end{itemize}
    \end{itemize}
    \item uczenie nienadzorowane -
    w przeciwieństwie do uczenia nadzorowanego, opiera się na braku
    nauczyciela, a zadaniem maszyny jest znalezienie wzorców i zależności między analizowanymi
    obiektami samodzielnie. Dwie metody które są najczęściej wykorzystywane w uczeniu nienadzorowanym
    to:
    \begin{itemize}
        \item analiza składowych głównych - polega na zmniejszaniu wymiarowości danych poprzez
        odnajdywanie, a następnie odrzucanie cech, które niosą ze sobą najmniejszą ilość informacji,
        \item analiza skupień (Klasteryzacja) - pozwala na identyfikację oraz grupowanie danych
        podobnych, które nie są w żaden sposób oznaczone. Może także pomóc w odnalezieniu anomalii
        nie pasujących do żadnej z wydzielonych grup.
        
    \end{itemize} 
    Wykorzystanie tego typu algorytmów pozwala na badanie danych nieoznaczonych,
    które są znacznie częściej spotykane niż dane oznaczone.
    
    Przykładowe zastosowania:
    \begin{itemize}
        \item redukcja wymiarów:
        \begin{itemize}
            \item wizualizacja danych ``big data'',
            \item kompresja danych.
        \end{itemize}
        \item klasteryzacja:
        \begin{itemize}
            \item spersonalizowane reklamy,
            \item systemy rekomendacyjne.
        \end{itemize}
    \end{itemize}
\end{itemize} 
\section{Algorytmy klasyfikacji}
Z problemem klasyfikacji można się spotkać wszędzie tam, gdzie wykorzystując
zbiór zmiennych objaśniających należy wskazać wartość przyjmowaną przez zmienną
modelowaną. W problemach klasyfikacji zmienna modelowana może przyjmować wartości
binarne (klasyfikacja dwuklasowa) lub jedną z wielu etykiet (klasyfikacja wieloklasowa).
Aby wybrać odpowiedni do rozwiązywanego problemu algorytm klasyfikacji należy wziąć 
pod uwagę cztery czynniki:
\begin{itemize}
    \item złożoność czasowa - jak długo trwa uczenie oraz predykcja nowych danych,
    \item interpretowalność - jak łatwo można wytłumaczyć decyzję podjętą przez 
    system,
    \item skalowalność - jak dużą ilość zasobów zużywa dany algorytm,
    \item czynnik ludzki - aby poprawnie ustawić parametry algorytmu potrzebna
    jest wiedza na temat jego działania, ponieważ poprawne ich ustawienie może 
    mieć większe znaczenie dla efektywności niż dobór najlepszego algorytmu. Często
    lepiej jest wykorzystać algorytm znany, a nie optymalny.
\end{itemize}
W dalszej części rodziału opisane zostały najpopularniejsze metody klasyfikacji.
\subsection{\textit{k}-NN}

Algorytm K najbliższych sąsiadów (K nearest neighbours) jest algorytmem
stosowanym zarówno w problemach regresyjnych, jak i klasyfikacji. Kroki jego działania
można opisać w następujący sposób:
\begin{itemize}
    \item umieszczenie wszystkich obiektów posiadających N cech jako punkty w N-wymiarowej przestrzeni,
    \item obliczenie odległości między obiektem, którego etykieta będzie przewidywana, a każdym innym
    obiektem,
    \item przypisanie do obiektu etykiety, którą posiada większość z K najbliższych obiektów. 
\end{itemize}
Faza uczenia w \textit{k}-NN polega wyłącznie na wczytaniu danych treningowych do pamięci, przez co jest 
ona bardzo szybka.

Najważniejszą kwestią w poprawnym implementowaniu algorytmu \textit{k}-NN jest odpowiednie wybranie liczby
K, której optymalna wartość będzie się różnić w zależności od danych. W problemach 
klasyfikacji często wybiera się K o wartości nieparzystej aby uniknąć problemu remisu
podczas zliczania sąsiadów. W przypadku problemów regresyjnych zamiast wykonywać głosowanie,
predykcję wykonuje się na podstawie średniej wartości K najbliższych sąsiadów ~\cite{MLAlgorithms}.

\begin{table}[h]
    \begin{tabularx}{\linewidth}{>{\parskip1ex}X@{\kern4\tabcolsep}>{\parskip1ex}X}
    \toprule
    \hfil\bfseries Zalety
    &
    \hfil\bfseries Wady
    \\\cmidrule(r{3\tabcolsep}){1-1}\cmidrule(l{-\tabcolsep}){2-2}
    
    %% PROS, seperated by empty line or \par
    prosty do zrozumienia i interpretacji,\par
    szybkość fazy uczenia,\par
    nie wykonuje generalizacji danych, przez co jest efektywny w przypadku danych nieliniowych.\par
    &
    
    %% CONS, seperated by empty line or \par
    przechowuje wszystkie dane treningowe w pamięci, co skutkuje wysoką złożonością pamięciową,\par
    wrażliwy na ilość danych i nieistotne cechy,\par
    kosztowny obliczeniowo.\par
    \\\bottomrule
    \end{tabularx}
    \caption{Wady i zalety \textit{k}-NN}
\end{table}
    

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./Img/KNN.png}
    \caption{Graficzne przedstawienie algorytmu \textit{k}-NN Źródło: Własne}
\end{figure}

\subsection{SVM}

Klasyfikator SVM (Support vector machines) można wykorzystywać zarówno 
w problemach regresyjnych i klasyfikacyjnych. Służą one do klasyfikacji binarnej, co 
oznacza, że obiekty należy podzielić na dokładnie dwie klasy. SVM polega na znalezieniu
takiej prostej lub płaszczyzny (w zależności od liczby cech), która w jak najlepszy sposób
dzieli obiekty dwóch klas. Czasami idealny podział jest niemożliwy, z czym klasyfikator 
ten radzi sobie w jeden z dwóch sposobów:
\begin{itemize}
    \item ignorowanie punktów, które uniemożliwiają podział,
    \item wykorzystanie tak zwanych kernel trick, które przekształcają dane do wyższego 
    wymiaru w którym podział jest możliwy.
\end{itemize}
Aby odnaleźć optymalne rozwiązanie SVM skupia się na punktach jak najbardziej skrajnych
obu klas, są one nazywane wektorami nośnymi. Zmiana ich położenia lub usunięcie
całkowicie zmienia położenie płaszczyzny~\cite{MLAlgorithms}.

\begin{table}[h]
    \begin{tabularx}{\linewidth}{>{\parskip1ex}X@{\kern4\tabcolsep}>{\parskip1ex}X}
    \toprule
    \hfil\bfseries Zalety
    &
    \hfil\bfseries Wady
    \\\cmidrule(r{3\tabcolsep}){1-1}\cmidrule(l{-\tabcolsep}){2-2}
    
    %% PROS, seperated by empty line or \par
    efektywny w wielowymiarowych przestrzeniach,\par
    działa nawet w przypadku, gdzie liczba danych treningowych jest mniejsza od
    ilości ich cech,\par
    przechowuje w pamięci tylko niewielką ilość danych treningowych.\par
    
    &
    
    %% CONS, seperated by empty line or \par
    nie jest on przystosowany do dużej ilości danych treningowych,\par
    nie radzi sobie dobrze jeżeli obiekty różnych klas nachodzą na siebie\par
    trudne do zinterpretowania wyniki predykcji.
    
    \\\bottomrule
    \end{tabularx}
    \caption{Wady i zalety SVM}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./Img/SVM.png}
    \caption{Wizualizacja algorytmu SVM Źródło: Własne}
\end{figure}

\subsection{MLP}

Działanie algorytmu Perceptronu wielowarstwowego (Multi layered perceptron) 
opiera się na wykorzystaniu modelu sztucznego neuronu, który na podstawie określonej
funkcji aktywacji oblicza na wyjściu pewną wartość na podstawie ważonych sum danych
wejściowych. 

Funkcje aktywacji dzieli się na:
\begin{itemize}
    \item funkcje progowe z wyjściem binarnym,
    \item funkcje liniowe z wyjściem ciągłym,
    \item funkcje nieliniowe z wyjściem ciągłym.
\end{itemize}
W sieciach wielowarstwowych najczęściej wykorzystuje się funkcje nieliniowe ponieważ 
wykazują one największe zdolności do nauki.

Perceptron wielowarstwowy składa się z trzech warstw sztucznych neuronów: 
\begin{itemize}
    \item warstwy wejściowej - są to neurony zapewniające całej sieci
    informacje, czyli zazwyczaj dane treningowe. Nie wykonują one żadnych obliczeń,
    a jedynie przekazują informacje do kolejnych warstw,
    \item warstwy ukrytej - wykonują one odpowiednie obliczenia zmieniając 
    i przekazując informacje z warstwy wejściowej do wyjściowej. Sieć może składać
    się z dowolnej liczby warstw ukrytych,
    \item warstwy wyjsciowej - wykonują one ostatnie obliczenia na danych, 
    a następnie zwracają wynik.
\end{itemize}

Uczenie takiej sieci polega na modyfikacji wag na wejściu neuronów, aby poprawić
wyniki klasyfikacji. Podczas gdy perceptron jednowarstwowy, czyli nieposiadający żadnej warstwy ukrytej może 
nauczyć się wyłącznie funkcji liniowych perceptron wielowarstwowy pozwala na nauczenie
się skomplikowanych funkcji nieliniowych~\cite{MLAlgorithms}.

\begin{table}[h]
    \begin{tabularx}{\linewidth}{>{\parskip1ex}X@{\kern4\tabcolsep}>{\parskip1ex}X}
    \toprule
    \hfil\bfseries Zalety
    &
    \hfil\bfseries Wady
    \\\cmidrule(r{3\tabcolsep}){1-1}\cmidrule(l{-\tabcolsep}){2-2}
    
    %% PROS, seperated by empty line or \par
    szybkie wykonywanie predykcji po nauczeniu modelu,\par
    efektywny dla nieliniowych danych posiadających wiele cech takich jak zdjęcia,\par
    działają najlepiej dla dużej ilości danych treningowych.\par
    &
    
    %% CONS, seperated by empty line or \par
    brak możliwości interpretacji wyniku predykcji,\par
    uczenie bardzo złożone obliczeniowo, co skutkuje długim czasem uczenia,\par
    użytkownik ma niewielki wpływ na działanie sieci.\par
    \\\bottomrule
    \end{tabularx}
    \caption{Wady i zalety MLP}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./Img/MLP.png}
    \caption{Graficzne przedstawienie perceptronu wielowarstwowego Źródło: Własne}
\end{figure}

\subsection{Decision trees}

Algorytm Decision trees polega na stworzeniu drzewa decyzyjnego, w którym korzeń i węzły
są poszczególnymi cechami zbioru danych, a liście odpowiadają klasom, które tym danym 
należy przypisać, ostatnim elementem są krawędzie, którymi reprezentuje się wartości cech. 
Kroki algorytmu są następujące:
\begin{enumerate}
    \item wybranie najlepszej cechy,
    \item dodanie gałęzi odpowiadających poszczególnym wartościom wybranej cechy,
    \item podział zbioru danych na podzbiory zgodnie z wartościami cechy,
    \item jeżeli wszystkie elementy podzbiorów należą do tej samej klasy 
    zakończenie gałęzi liściem. W przeciwnym wypadku powtórzenie kroków 
    od 1 do 4 dla każdego podzbioru.
\end{enumerate}
Aby dokonać optymalnego podziału algorytm musi wybrać cechy powodujące jak najlepsze
podzielenie różnych klas, a więc niosące ze sobą największą ilość informacji.
Wykorzystuje się w tym celu entropię, której wartość pozwala określić średnią ilość 
informacji niesioną przez poszczególne cechy~\cite{MLAlgorithms}.
\begin{equation}
    H(x)=-\sum_{i=1}^n p(x_i) \log_2 p(x_i).
    \label{eq-entropy}
\end{equation}
Wzór na entropię ~\ref{eq-entropy}, gdzie \textit{p(x\textsubscript{i})} jest prawdopodobieństwem
wybrania klasy \textit{x\textsubscript{i}}
\begin{table}[h]
    \begin{tabularx}{\linewidth}{>{\parskip1ex}X@{\kern4\tabcolsep}>{\parskip1ex}X}
    \toprule
    \hfil\bfseries Zalety
    &
    \hfil\bfseries Wady
    \\\cmidrule(r{3\tabcolsep}){1-1}\cmidrule(l{-\tabcolsep}){2-2}
    
    %% PROS, seperated by empty line or \par
    łatwe do zinterpretowania wyniki oraz wygodne do zwizualizowania,\par
    automatyczna selekcja ważnych cech,\par 
    obecność cech zbędnych nie ma wpływu na poprawność,\par
    szybkie wykonywanie predykcji.\par
    &
    
    %% CONS, seperated by empty line or \par
    wrażliwy na przeuczenie,\par
    potrzeba stworzenia nowego drzewa podczas dodawania nowych danych,\par
    małe zmiany w danych treningowych maja duży wpływ na predykcję.\par
    
    \\\bottomrule
    \end{tabularx}
    \caption{Wady i zalety Drzew decyzyjnych}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./Img/BinaryTree.png}
    \caption{Przykładowe drzewo decyzyjne podejmujące decyzję czy wyjść na zewnątrz Źródło: Własne}
\end{figure}

\subsection{RF}
Algorytm lasów losowych (Random forest) w skrócie RF, polega na stworzeniu lasu składającego się z drzew decyzjnych, 
które są budowane w następujący sposób:
\begin{itemize}
    \item z danych treningowych wybrane zostają losowe obiekty (może nastąpić wybranie tego samego obiektu dwukrotnie),
    \item wybrane obiekty stają się nowym zbiorem treningowym,
    \item w nowym zbiorze wybrana zostaje określona liczba losowych cech,
    \item tworzone zostaje drzewo decyzjne oparte tylko i wyłącznie na wybranych wcześniej cechach
    \item poprzednie kroki są powtarzane do czasu stworzenia określonej liczby drzew zazwyczaj jest to 
    liczba między 64 a 128 drzewami. 
\end{itemize}
Po stworzeniu wielu drzew decyzyjnych model jest gotowy aby wykonywać predykcje.
Predykcja w algorytmie RF jest wykonywana na podstawie głosowania przez każde stworzone drzewo na klasy, w głosowaniu 
tym wybrana zostaje klasa, na którą głosowała większość drzew ~\cite{randomforest}.

\begin{table}[H]
    \begin{tabularx}{\linewidth}{>{\parskip1ex}X@{\kern4\tabcolsep}>{\parskip1ex}X}
    \toprule
    \hfil\bfseries Zalety
    &
    \hfil\bfseries Wady
    \\\cmidrule(r{3\tabcolsep}){1-1}\cmidrule(l{-\tabcolsep}){2-2}
    
    %% PROS, seperated by empty line or \par
    odporny na overfitting,\par
    pojawienie się nowych danych nie wymaga tworzenia modelu od nowa,\par
    zachowuje poprawność w przypadku brakujących danych.\par
    
    &
    
    %% CONS, seperated by empty line or \par
    ciężkie do zinterpretowania wyniki z powodu ilości drzew,\par
    długi czas uczenia,\par
    nie sprawdza się dobrze w przypadku problemów regresyjnych\par
    
    \\\bottomrule
    \end{tabularx}
    \caption{Wady i zalety RF}
\end{table}

\subsection{Naive Bayes}

Naiwny klasyfikator Bayesowski (Naive Bayes) jest probabilistycznym klasyfikatorem, który
zakłada wzajemną niezależność wszystkich cech, stąd naiwność algorytmu. Stworzony przez niego model
opiera się na obliczeniu prawdopodobieństw wystąpienia poszczególnych cech pod warunkiem, że 
występują one w danej klasie. Pozwala to przy użyciu twierdzenia Bayesa ~\ref{eq:bayes} obliczyć prawdopodobieństwa 
warunkowe określające z jak dużą pewnością występująca kombinacja cech prowadzi do 
wystąpienia różnych klas, na podstawie czego algorytm wykonuje predykcję~\cite{MLAlgorithms}. 


\begin{equation}
    \label{eq:bayes}
    P(A|B) = \frac{P(A|B)P(A)}{P(B)}
\end{equation}
gdzie P(A) i P(B) prawdopodobieństwa wystąpienia zdarzeń A i B, P(A|B) prawdopodobieństwo wystąpienia
zdarzenia A pod warunkiem wystąpienia zdarzenia B.

Algorytm ten jest często wykorzystywany w systemach czasu rzeczywistego, ponieważ działa bardzo szybko jednak
jego poprawność w porównaniu do bardziej skomplikowanych algorytmów jest mniejsza.
\begin{table}[h]
    \begin{tabularx}{\linewidth}{>{\parskip1ex}X@{\kern4\tabcolsep}>{\parskip1ex}X}
    \toprule
    \hfil\bfseries Zalety
    &
    \hfil\bfseries Wady
    \\\cmidrule(r{3\tabcolsep}){1-1}\cmidrule(l{-\tabcolsep}){2-2}
    
    %% PROS, seperated by empty line or \par
    bardzo szybki nawet w przypadku przewidywania wieloklasowego,\par
    działa nawet dla małej ilości danych,\par
    dobra poprawność w przypadku klasyfikacji danych wielowymiarowych
    jak na przykład w przypadku klasyfikacji tekstu.\par
    
    &
    
    %% CONS, seperated by empty line or \par
    jeżeli pewna cecha nie pojawi się w danych treningowych, a istnieje w danych 
    testowych algorytm nie będzie w stanie wykonać jego predykcji,\par
    przyjmuje niezależność cech co w rzeczywistości jest prawie niespotykane,\par
    mniejsza poprawność w porównaniu z bardziej skomplikowanymi algorytmami.\par
    
    \\\bottomrule
    \end{tabularx}
    \caption{Wady i zalety Naive Bayes}
\end{table}

\section{Zagrożenia}
Ogromny rozwój uczenia maszynowego i sztucznej inteligencji spowodował, że znalazły one 
wiele zastosowań w różnych dziedzinach życia codziennego. Powoduje to, że wiele osób skupia się 
na pozytywach związanych
z tymi technologiami, jednakże niektórzy dostrzegają ogromne zagrożenia płynące z niewłaściwego 
wykorzystania ich i katastrofalne skutki, do których mogą doprowadzić. 
Wśród takich osób znajdują się naukowcy tacy jak jak Stephen Hawking, Elon Musk, Steve Wozniak i Bill Gates. 
Według Elona Muska sztuczna inteligencja niesie ze sobą większe zagrożenie niż bomby atomowe ~\cite{dangers}. 

Dwa główne sposoby w jaki sztuczna inteligencja może powodować zagrożenie to:
\begin{itemize}
    \item AI zaprogramowane do czynienia krzywdy, czyli na przykład wykorzystanie uczenia maszynowego
    w takich narzędziach jak broń, która w niewłaściwych rękach stanowiłaby zagrożenie dla całej ludzkości,
    \item AI zaprogramowane w dobrych celach, jednak odnajdujące krzywdzący innych optymalny
    sposób osiągnięcia sukcesu, może to być związane ze złym przekazaniem celu jaki maszyna ma osiągnąć przez 
    programistę. Przykładowo mógłby to być samojezdny samochód, który podczas jazdy zwraca uwagę tylko i wyłącznie 
    na to by jak najszybciej dojechać do celu podróży. Samochód taki nie przestrzegał by istniejących praw drogowych
    ponieważ ograniczałyby one jego prędkość.
\end{itemize}
Przykłady te pokazują, że zagrożenie nie płynie z sytuacji gdzie maszyny odwracają się przeciwko ludzkości, a 
z sytuacji w której maszyny wykonują poświęcone im zadania za dobrze i nie w sposób przewidziany przez 
stwórcę systemu.

Istotnym problemem jest także bezrobocie będące skutkiem zastępowania ludzi maszynami,
ponieważ są one znacznie tańsze w utrzymaniu niż pracownicy, a z wykorzystaniem uczenia
maszynowego mogą one opanować niektóre zadania lepiej od ludzi.
Rozwiązaniem na takie problemy mogłyby być obowiązkowo przestrzegane zasady bezpiecznego korzystania
ze sztucznej inteligencji, których złamanie wiązałoby się z odpowiedzialnością karną.