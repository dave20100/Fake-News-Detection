\chapter{Projekt i implementacja systemu}
Do wykonania badań efektywności algorytmów uczenia maszynowego w rozpoznawaniu
fałszywych informacji potrzebne było stworzenie systemu, który w prosty sposób 
dla każdego z badanych algorytmów wykonałby uczenie go na danych treningowych 
a następnie sprawdził jak dobrze przewiduje on przypadki ze zbioru testowego.
System musi także w odpowiedni sposób przygotować dane tekstowe przy użyciu takich
metod jak zamiana dużych liter na małe oraz usuwanie znaków interpunkcyjnych itd.
Ważną funkcjonalnością oprogramowania jest też to by dzielił on dane tekstowe na 
różnej długości NGramy. 

NGramy są to sekwencje następujących po sobie jednostek, którymi mogą być słowa, 
głoski lub litery. W pracy wykonane zostaną badania różnych algorytmów klasyfikacji
na podstawie podziału na różnej długości literowe NGramy a następnie wyniki zostaną
porównane w celu odnalezienia optymalnych ustawień do rozpoznawania Fake newsów.

System pozwoli na zbadanie takich wyników algorytmów jak efektywność predykcji oraz 
czasy trwania poszczególnych faz.

\section{Wykorzystane technologie}
Najpopularniejszymi językami programowania wykorzystywanymi do tworzenia modeli uczenia
maszynowego są:
\begin{itemize}
    \item Python,
    \item R,
    \item Lisp,
    \item Prolog.
\end{itemize}
Do wykonania systemu został wybrany język Python w wersji 3.8.5, jest to interpretowalny
język wysokiego poziomu powstały w roku 1991. Został on wybrany 
ponieważ posiada on dużą ilość bibliotek takich jak: 
\begin{itemize}
    \item Scikit-learn,
    \item PySpark,
    \item PyTorch,
    \item TensorFlow,
    \item Keras.
\end{itemize}
Znacznie ułatwiają one korzystanie z możliwości uczenia maszynowego.
W pracy zostanie wykorzystana biblioteka scikit-learn w wersji 0.23 zawiera ona wszystkie 
testowane algorytmy i pozwala na bardzo wygodną implementację ich. Posiada także obie 
badane metody wektoryzacji tekstu czyli Bag of words oraz TFIDF. Jest ona udostępniana na zasadach licencji BSD ~\cite{scikitlearn}. 
Licencja ta pozwala na użytkowanie i redystrybucję kodu zarówno zmodyfikowanego jak i oryginalnego.

Ostatnią biblioteką potrzebną do stworzenia systemu była Natural Language Toolkit nazywana w skrócie 
NLTK, jest to biblioteka służąca do przetwarzania języków naturalnych, która pozwala na proste przygotowanie tekstu do jego analizy
\section{Wymagania funkcjonalne}
Aplikacja powinna spełniać następujące wymagania funkcjonalne: 
\begin{itemize}
    \item Wczytywać dane z zewnętrznego pliku w formacie csv,
    \item W odpowiedni sposób przygotowywać dane,
    \item Dzielić dane na treningowe oraz testowe,
    \item Wykonywać uczenie badanych algorytmów na danych treningowych,
    \item Testować nauczone modele na danych testowych,
    \item Zapisywać wyniki badań do pliku w formacie csv,
\end{itemize}
Są to funkcjonalności kluczowe aby system pozwalał wykonywać ważne dla osiągnięcia celu
badania. 
\section{Implementacja}
Implementacja składa się z dwóch funkcji służących do przygotowywania danych tekstowych
do późniejszej analizy oraz jednej pętli głównej, która wykonuje wszystkie badania
a następnie zapisuje je do pliku w formacie csv. 

W systemie tworzony jest także obiekt
wielowymiarowej mapy który, po zakończeniu wykonywania programu przechowuje
wszystkie nauczone modele wektoryzacji oraz uczenia maszynowego. Zapisanie go za pomocą
biblioteki takiej jak \textit{Pickle} pozwala
w prosty sposób użyć i przetestować modele na dowolnych danych testowych. 


\begin{lstlisting}[language=Python, caption={Funkcja przygotowywująca dane pobrane z pliku csv}, captionpos=b, frame=single]
def basicPreparation(fileName): 
    file = pd.read_csv(fileName)
    label_encoder = preprocessing.LabelEncoder()
    file['label'] = label_encoder.fit_transform
        (file['label']) 
    file = file.applymap
        (lambda s: s.lower() if type(s) == str else s) 
    return (file['text'], file['label'])
\end{lstlisting}
Pierwsza funkcja \textit{basicPreparation} przyjmuje tylko jeden argument, którym jest
nazwa pliku. Metoda ta wczytuje do pamięci dane z pliku csv a następnie wykorzystuje
tzw. LabelEncoder, który pozwala na wykonanie zamiany etykiet w zbiorze danych na formę
numeryczną, w tym przypadku zamienia ona wartości REAL i FAKE na 0 i 1. Ostatnią 
akcją wykonywaną przez funkcję jest zamiana wszystkich liter zawartych w danych na małe.

Funkcja ta zwraca krotkę zawierającą dwa elementy:
\begin{itemize}
    \item Listę artykułów
    \item Listę etykiet
\end{itemize} 

\begin{lstlisting}[language=Python, caption={Funkcja przygotowywująca dane tekstowe}, captionpos=b, frame=single]
def dataPreprocessing(articles, labels):
    deletionIndexes = []
    for articleIndex in range(len(articles)):
        articles[articleIndex] = 
            ''.join(w+' ' for w in articles[articleIndex]
            .split(' ') if not w in stop_words and w != '')  
        articles[articleIndex] = 
            re.sub(r'[^a-zA-Z]+', ' ', articles[articleIndex])
        articleLength = len(articles[articleIndex])
        if(articleLength == 0 
            or articleLength < 500 
            or articleLength > 5000):
            deletionIndexes.append(articleIndex) 
    articles = articles.drop(deletionIndexes, axis=0)
    labels = labels.drop(deletionIndexes, axis=0)
    return (articles, labels)

\end{lstlisting}

Drugą funkcją znajdującą się w programie jest \textit{dataPreprocessing} posiada 
ona dwa argumenty:
\begin{itemize}
    \item articles - Lista artykułów
    \item labels - Lista etykiet
\end{itemize}
Funkcja ta wykonuje przygotowanie tekstu do późniejszej wektoryzacji. Wykonuje ona 
kolejno operacje:
\begin{enumerate}
    \item Usunięcia Stop words z tekstu każdego artykułu,
    \item Usunięcia wszystkich znaków nie będących literami,
    \item Usunięcia ze zbioru danych artykułów o długości poniżej 500 znaków oraz powyżej 5000 znaków 
    ponieważ przekazują one niepotrzebną ilość informacji.
\end{enumerate}
Funkcja ta zwraca dokładnie taką samą krotkę jak \textit{basicPreparation}.


Główna pętla programu wykonuje kolejno następujące operacje:
\begin{enumerate}
    \item Wytrenowanie modelu wektoryzatora na danych treningowych,
    \item Wektoryzacja danych treningowych i testowych za pomocą 
    nauczonego wcześniej modelu,
    \item Normalizacja cech za pomocą StandardScaler,
    \item Trening algorytmu na cechach treningowych funkcją \textit{fit},
    \item Obliczenie efektywności funkcją \textit{score} na danych testowych,
    \item Zapisanie wyników do pliku csv o tytule odpowiadającym nazwie algorytmu 
    uczenia maszynowego.
\end{enumerate}
Aby obliczyć czas trwania faz uczenia i predykcji wykorzystano bibliotekę time w celu
zapisania czasu przed i po wykonaniu każdej z faz a następnie odjęciu tych wartości.